---
title: Replication of 'Data quality of platforms and panels for online behavioral
  research' by Peer et al. (2021, Behavior Research Methods)
author: "Kimia Saadatian (kimia@stanford.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: no
  pdf_document:
    toc: yes
    toc_depth: '3'
  word_document:
    toc: yes
    toc_depth: '3'
---

## Introduction

### Justification for choice of study

I am replicating a recent study by Peer et al. (March, 2021), in which they compared quality of data gathered from the most popular sites used for online participant recruitment — Amazon MTurk, CloudResearch (formerly TurkPrime), and Prolific. Peer et al. (2021) found "higher data quality for Prolific and CloudResearch compared to MTurk (the differences between Prolific and MTurk and between CR and MTurk were significant (p < .001), but the difference between CR and Prolific was not (p = 0.91))."


In response, associates of the site CloudResearch, Litman et al. (2021, SSRN), attempted to replicate Peer et al.'s (2021) findings and found "undisclosed methodological decisions that would limit the inferences made in [the original publication]" (Litman et al., 2021). The team from CloudResearch were specifically concerned that Peer et al. (2021) "chose to turn off the recommended data quality filters and reputation qualifications, including filters that are on by default and were designed to address known data quality issues on MTurk." Contrary to the findings from Peer et al., (2021), upon implementing what they claim are recommended and default filters on MTurk, the team at CloudResearch (Litman et al., 2021) found CloudResearch’s  data quality superior to that of Prolific. 

I am curious to examine the data quality filters that were used in each of these studies and investigate these sites' data quality as a relatively-objective third-party investigator.

### Anticipated challenges

The only challenges I anticipate at this stage is tracking and figuring out the specific automated/default, and manually-set data quality filters that each team of researchers incorporated in their data collection process. 


### Links

Project repository: https://github.com/psych251/saadatian2021.git

Survey preview: https://ucbpsych.qualtrics.com/jfe/preview/SV_cSKr5voz0akZ7P8?Q_CHL=preview&Q_SurveyVersionID=current

Project OSF: https://osf.io/qw4sf/?view_only=49333769f6464316a805d48e8cd1d5ca

Original paper: https://github.com/psych251/saadatian2021/blob/3c2eeb3b8a352c51dea439f7878b2ab95f67acff/original-paper/Peer%20et%20al.%20(2021).pdf

Original study's OSF: https://osf.io/342dp/

## Methods

### Power Analysis

Here I aim to replicate the main finding in Study 2, where Peer et al., (2021) compare data quality across the three sites and find "statistically significant differences between the sites on [overall data quality score], F(2, 1458) = 129.4, p < .001, which showed higher scores for Prolific and CR (M = 5.87, 5.78, SD = 1.0, 1.1, respectively) compared to MTurk (M = 4.55, SD = 1.9))".

To replicate an F statistic of 129.4 with a power of 0.95, I would need a sample of 43 participants minimum from each site. However, to get precise estimates of the differences in data quality across the three sites, I recruit 100 participants from each of the three platforms (Mturk, CloudResearch, and Prolific). Participants from these three independent samples should add up to a total of 300 participants for the study as a whole.


### Planned Sample

One hundred participants who are 18 years or older and ALSO current residents of the United States are recruited on Amazon MTurk, CloudResearch, and Prolific. Similar to the original study, participants are paid a $1.5 each for completing the study, which originally averaged around 9.8 minutes-long (SD = 5.2; Peer et al., 2021). 

#### Data Quality Pre-screening
The current study implements all data quality pre-screening filters that were used in the original study (Peer et al., 2021). Peer et al. (2021) describe the data quality pre-screening and exclusion criteria as:

1) "Restricting the study to participants with at least 95% approval rating and at least 100 previous submissions" for participants recruited from all three sites
2) Using the site setting to "block low data quality workers" on CloudResearch
3) Excluding Prolific participants who completed their Study 1 on Prolific
4) Excluding participants on CloudResearch completed their Study 1 on CloudResearch or MTurk

*Note: since they had to post the survey twice on Mturk (once through their MTurk account and once through
their CR account), some participants (N = 39) completed the survey twice. Peer et al. (2021) removed those participants' later submissions and I will be doing the same. 


Other necessary and recommended data quality filters mentioned by the team at CloudResearch are: 



In the current study, I apply the following data quality filters on all three sites:
*Used by Peer et al. (2021)*
1) MTurk - only workers who have completed 100 submissions or more and 95% of those submissions are eligible
2) CloudResearch - only workers who have completed 100 submissions or more and 95% of those submissions are eligible & 
the default setting is set to "block low data quality workers"
3) Prolific -  only workers who have completed 100 submissions or more and 95% of those submissions are eligible. 

*Used by Litman et al. (2021)*
4)

#### Exclusions
Similar to the original study (Peer et al., 2021), I will exclude participants who do not complete all of the study.


### Measures

#### Attention 
-> *[ac1, ac2, ac3]*

The original scholars measure attention using one two-item explicit question and one covert question. Similar to the original study, "the first [attention check] asks participants to answer "six" and "three" to two items regardless of their actual preference (other responses are coded as failures); the second is an item within a scale worded "I currently don't pay attention to the questions I'm being asked in the survey" (response other than "strongly disagree" is coded as a failure)."



#### Comprehension
-> *[comprehension1 and comprehension2]*

Comprehension is examined via coding of participants' written summaries of instructions to two tasks. "The first task asks participants to identify faces in a picture, but includes an instruction to only report zero; the second includes instructions for the "Matrix task" (Mazar et al., 2008)."In the original study, two independent, blind raters coded the participant responses. The investigators counted an answer as correct if both coders had coded it as correct, applying a third rater to responses with split votes.
Due to time constraints in data coding and analysis, the PI, Kimia Saadatian, is the sole rater of open responses to comprehension items. Following this class replication project, Kimia plans to have the data coded by two blind raters independently.



#### Reliability  
-> *[nfc1 to nfc18]*

Similar to the original study, reliability is measured using Cronbach's alpha for the eighteen-item, five-point Need for Cognition scale (Cacioppo et al., 1984). The original investigators chose this scale because it has been validated and found to be highly reliable across many studies.



#### Honesty 
-> *[honesty 1, honesty2, honesty3]*

Replicating the honesty measure in the original study, honesty is measured using "an online version of the Matrix task (Mazar et al., 2008) that include(s) two unsolvable matrices. Reporting solving any of these two problems will be coded as a dishonest response. Additionally, we examine whether participants lie about their eligibility for a future study by asking them to indicate if they want to be invited to a study that samples participants of their own gender but whose age will be described as 5-10 years above the age participants reported in the beginning of the study."



#### Data Quality
-> *[ac_total + comprehension_total + honesty_total]*

Data quality was computed by calculating composite score of data quality based on the average scores of attention, comprehension, and dishonesty.


#### Other measures
*The below measures are not immediately relevant to the main analyses reported in this class project report.*

To replicate the original study as closely as possible, we also measure participants' drop-out rates, response duration, overall response time and speed between sites, differences in responses to the NFC scale, demographics, and patterns of usage of the site (main purpose, frequency of usage, number of submissions and approval ratings, usage of other sites), and whether or not the participants have completed a similar study in the last months.


### Procedure	

Mimicking the original survey introduction (Peer et al., 2021), the description of the currect study states : "You are invited to complete a survey on individual differences in personal attitudes, opinions, and behaviors." 

I use the original .QSF file for the survey content, which begins with 
demographic questions, followed by the data quality measures described above. 

Participants are prompted with questions related to their usage of the online platform (e.g., how often they use the site, why they use it, how much they earn on average, their percent of approved submissions (responses that participants submit and are approved by the researcher), and how often (if at all) they use other sites) before finishing the survey and receiving their online payment.


#### Deviations from the Original Study 
: 1) not forced responses 2) original excludes those who dont complete the entire study but thats not reasonable so i probably wont

### Analysis Plan

The current study utilizes the same analyses that were conducted in the original study by Peer et al. (2021).

The main analysis I am replicating is from study 2, where Peer et al. (2021) found higher data quality scores for data obtained from Prolific and CloudResearch (M = 5.87, 5.78, SD = 1.0, 1.1, respectively) compared to MTurk (M = 4.55, SD = 1.9; F(2, 1458) = 129.4, p < .001).


In order to compute the overall data quality score, the original authors:

*Relevant for current EXPLORATORY analyses:*

- "Used chi-square tests to examine differences in the rates of attention, comprehension and dishonesty
- Computed average scores for (across the items per aspect)  
- Compared them between sites using ANOVA and regression analyses. 
- Tested for differences between reliability coefficients using Hakistan & Whalen (1976) method"

*Relevant for current CONFIRMATORY analyses:*

- "Computed an overall composite score of data quality (based on the average scores of attention, comprehension and dishonesty)
- Compare composite score of data quality between sites using ANOVA and regression."


### Differences from Original Study
I do not anticipate any deviations from the original *analysis plan* at this stage.


### Methods Addendum (Post Data Collection)

#### Actual Sample

#### Differences from pre-data collection methods plan


## Results

### Results from Peer et al. (2021)
The composite Data Quality score ranged from 0 to 7 (M = 5.41; SD = 1; Med = 6). Overall data quality composite scores ranked from highest to lowest were:
1) Prolific (M = 5.87, SD = 1.0) 
2) CR ( M = 5.78, SD =  1.1) 
3) MTurk (M = 4.55, SD = 1.9)

Peer et al. (2021) found statistically significant differences between the sites on Overall Data Quality Score, F(2, 1458) = 129.4, p < .001.

Post hoc tests with Bonferroni correction showed:
- differences between Prolific and MTurk p < .001 
- differences between CR and MTurk p < .001 
- difference between CR and Prolific  p = 0.91. 



### Data preparation

Data preparation following the analysis plan.

```{r, echo=FALSE}
### Fixing error with knitting
#### RMarkdown v1 used error = TRUE by default, but RMarkdown v2 uses error = FALSE.
knitr::opts_chunk$set(error = TRUE)
```

```{r, Data Preparation}
###Data Preparation
##### Starting a Script #####
  # Clear environment
    rm(list=ls())
  # Checking working directory
    getwd()

  #### Loading in data ####
      dat <- read.csv("pilotD_replication.csv",
                       na.strings="NA", strip.white=TRUE)
  ### Load Relevant Libraries and Functions
    library(tidyverse)
    library(dplyr)
    library(car)
    library(ggplot2)
    library(ltm)

##### Looking at your data #####
  #### Identifying problems ####
    # Finding the number of duplicate cases
      # WARNING: These do not include the first instance of the duplicated value.
      length(dat$id) - length(unique(dat$id))
    # Finding rows of duplicate cases. 
      # WARNING: These do not include the first instance of the duplicated value.
      which(duplicated(dat$column))
      
  #### Looking at the dataset ####
        colnames(dat) # list of column names in order
        head(dat) # first five rows
        View(dat) # view dataset
        nrow(dat) # number of rows in dataset
        ncol(dat) # number of columns in dataset

#### Data exclusion / filtering
  #### Removing unwanted data ####
    # Omitting rows with NAs
      data <- na.omit(dat)
    # Removing columns we don't need about for primary analyses
      dat <- dat %>% 
          dplyr::select(ac1, ac2, ac3, comprehension1, 
                        comprehension2, contains("honesty"),
                        ResponseId, site, sample
                        )
```
### Confirmatory analysis

I report differences between sites on each data quality measure and then aggregate those findings to a composite score of data quality, reporting differences across the three sites.

ATTENTION
(originally : ACQs, χ2(4) = 548.48, 203.56, p < .001. )

```{r, Attention}
#### recode ac1 so that : pass if  ac1 = "6" ; fail ifelse
#### record ac2 so that : pass if ac2 = "3" ; fail ifelse
#### recode ac3 so that :  pass if ac3 = '1' ; fail ifelse

dat <- dat %>% 
  mutate(ac1_pass = ifelse(ac1 == "6", 1,0),
         ac2_pass = ifelse(ac2 == "3", 1,0),
         ac3_pass = ifelse(ac3 == "1", 1,0),
         ac_total = (ac1_pass + ac2_pass + ac3_pass)
         )
```


COMPREHENSION
(originally : χ2(4) = 152.4, p < .001 )

```{r, Comprehension}
#### Have 2 coders code any response that suggested a minimum level of understanding as indicating comprehension and only flag responses as incorrect if they were undoubtedly illegible. Responses that are flagged by both raters will be coded as incorrect answers.
  ### manually code open responses to comprehension1
  ### manually code open responses to comprehension2

dat <- dat %>% 
  mutate(comprehension_total = (comprehension1 + comprehension2)
         )
```

HONESTY
(originally :  χ2(4) = 153.44, p < .001 )

```{r, Honesty}
#### code responses to honesty manually so that:
  ### honesty1 : 0 = not honest 1 = honest
  ### honesty2  : 0 = not honest 1 = honest
  ### honesty3 : 0 = not honest 1 = honest 2 = other

colnames(dat)
dat <- dat %>%
  mutate(honesty_total = (X1_honesty1 + X2_honesty1 +
                    X3_honesty1 + X4_honesty1 + X5_honesty1 +
                    honesty2)
         )
```


RELIABILITY
(originally : )

```{r, Reliability}
#### Reliability is measured using the cronbach alpha of the NFC scale
# The original authors re-coded the negatively worded items of the NFC before running analyses but here, our code specifically defines the reverse scored items within this scale.

dat <- dat %>% 
  mutate(nfc3_reversed = ifelse(ac1 == "6", 1,0),
         nfc4_reversed = ifelse(ac2 == "3", 1,0),
         nfc5_reversed = ifelse(ac3 == "1", 1,0),
         nfc7_reversed = ifelse(ac1 == "6", 1,0),
         nfc8_reversed = ifelse(ac2 == "3", 1,0),
         nfc9_reversed = ifelse(ac3 == "1", 1,0),
         nfc12_reversed = ifelse(ac1 == "6", 1,0),
         nfc16_reversed = ifelse(ac2 == "3", 1,0),
         nfc17_reversed = ifelse(ac3 == "1", 1,0),
         nfc_total = (nfc1 + nfc2 + nfc3_reversed +
                        nfc4_reversed + nfc5_reversed +
                        nfc6 + nfc7_reversed + nfc8_reversed +
                        nfc9_reversed + nfc10 + nfc11 +
                        nfc12_reversed + nfc13 + nfc14 +
                        nfc15 + nfc16_reversed + nfc17_reversed +
                        nfc18
                      )
         )

cronbach.alpha(nfc_total)
```

OVERALL DATA QUALITY SCORES
(From the original study: "The score gave participants a value between 0 and 5, showing whether they passed one or both ACQs, answered correctly one or two comprehension questions,.. claim to have solved the unsolvable problem, [and claimed to qualify for a future study that they did not qualify for based on their earlier responses in the survey]. ... The overall composite score should not be considered as measuring the same construct. Rather, it is used here as a multifactorial measure that attests to the overall general level of data quality")

```{r, Data Quality}
dat <- dat %>%
  mutate(dataquality = (ac_total + comprehension_total +
                           honesty_total)
          )

colnames(dat)
```

LOOKING AT ALL MEANS ON EACH PLATFORM
```{r }
dat %>% 
  group_by(sample) %>%
  summarize(dataquality, na.rm = T) # the na.rm tells R to ignore NA values
```

COMPARING DATA QUALITY ACROSS THE THREE GROUPS
```{r, Data Quality Across Platforms}
# Compute one-way ANOVA
singleANOVA <- aov(dataquality ~ sample, data = dat)
# Summary of one-way ANOVA
summary(singleANOVA)
```

*Side-by-side graph with original graph is ideal here*

-----

### Exploratory analyses


## Discussion

### Summary of Replication Attempt

-> a paragraph summarizing the primary result from the confirmatory analysis and the assessment of whether it replicated, partially replicated, or failed to replicate the original result.  

### Commentary

-> add commentary reflecting about:
a) insights from follow-up exploratory analysis
b) assessment of the meaning of the replication (or not) 
- e.g., for a failure to replicate, are the differences between original and present study ones that definitely, plausibly, or are unlikely to have been moderators of the result
c) discussion of any objections or challenges raised by the current and original authors about the replication attempt.
